{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 客户交易行为分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 客户交易行为分析\n",
    "在上一个环节，我们已经对客户交易数据进行了异常值、缺失值和重复值的处理，同时也对时间和交易金额进行了相应的转换。在本环节，我们将利用Python中的绘图库(如Seaborn、Matplotlib)，通过可视化的方式，对客户交易行为进行分析。对客户交易行为进行分析的目的是统计和观察在各个维度上的数据分布，得到一些关于客户行为的有价值的结论。具体地，客户交易行为分析的流程如图4.1所示。\n",
    "\n",
    "<center><img src=\".\\Pics\\Pic4_1.jpg\" width=600 height=200 alt=\"cursor\" align=center /></center>\n",
    "<center><font size=\"2.5px\" face=\"微软雅黑\" color=\"666666\">图4.1：数据交易行为分析流程</font></center>\n",
    "\n",
    "下面我们对图4.1中的每一个流程进行简要说明。\n",
    "\n",
    "+ 时间维度的分析：对交易时间进行分析，探索交易随时间的分布规律。例如分析交易次数随时间的变化，不同时段的交易次数分布等。\n",
    "+ 交易属性的分析：对交易金额和次数进行分析，例如分析客户的交易次数分布和平均交易金额分布等。\n",
    "+ 文本数据预处理：为了便于后续分析，对交易附言的文本进行预处理，例如过滤特殊符号和文本分词等。\n",
    "+ 文本数据的分析：对预处理后的文本进行分析，例如设置停用词、统计词频、绘制词云分布图和提取关键词等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 交易次数随时间的可视化分析\n",
    "在数据集的描述中，我们已知交易数据的时间跨度为5年。对数据进行探索分析时，我们首先分析这5年跨度的总交易次数分布，绘制总交易次数随时间变化的折线分布图。\n",
    "\n",
    "Pandas的绘图机制封装在Matplotlib.pyplot之上，首先创建画布`figure`对象，在`figure`上进行绘图。Series和DataFrame对象都有一个用于生成各类图表的`plot()`函数，默认情况下生成的是折线图。Series的索引会传给Matplotlib，用于绘制`x`轴。语法如下：\n",
    "\n",
    "`Series.plot(kind='line', title=None, legend=False, style=None,rot=None)`\n",
    "+ `kind`：画图的种类，默认为`line`(折线图)，可以设置为其他，如`bar、hist、pie`分别代表柱状图、直方图和饼图。\n",
    "+ `title`：设置图像标题，默认为不设置。\n",
    "+ `legend`：设置图例，默认为不设置。\n",
    "+ `style`：设置折线的类型，默认为不设置。\n",
    "+ `rot`：对`x`轴刻度进行旋转，如`rot=45`，则代表旋转`45`度，默认为不设置。\n",
    "\n",
    "基于Matplotlib.pyplot机制，可以再使用其中的一些方法对图像进行设置，如设置坐标轴标签(`plt.xlabel、plt.ylabel`)、图的标题(`plt.title`)等等。\n",
    "\n",
    "在前面我们已经将Unix时间戳转换为了标准的时间格式并进行了时区转换，保存在`pay_time`列中，此列的数据格式为`datetime64`。在NumPy中，时间格式的数据类型称为`datetime64`。Pandas是处理时间序列的利器，有着强大的日期数据处理功能，继承和使用了`datetime64`格式，可以按日期筛选数据、按日期显示数据、按日期统计数据。\n",
    "\n",
    "Series对象具有简洁的返回时间及日期的访问器`Series.dt`，如`Series.dt.date`、`Series.dt.month`和`Series.dt.year`分别代表提取时间中的天、月、年。\n",
    "\n",
    ">实训任务\n",
    ">+ 请使用`value_counts()`函数统计`data`中每天的交易次数，接着使用Pandas绘图机制，调用`plot()`函数绘制每天的交易次数随时间的折线分布图。\n",
    ">  + 提示：原始`data['pay_time']`精确到秒，使用`data['pay_time'].dt.date`可以提取日期中的天，再调用`value_counts()`即可统计每天的交易次数。\n",
    ">+ 请设置`x`轴标签为`时间`，设置`y`轴标签为`交易次数`。\n",
    ">+ 请设置图的标题为`不同时间的交易次数分布`。\n",
    "\n",
    "可以从折线分布图中看到交易的发生时间主要集中在2016年7月至2017年12月，在本项目后续环节，我们选取该时间段的交易记录进行重点分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "# 绘制折线图\n",
    "data['pay_time'].dt.date.value_counts().plot()\n",
    "\n",
    "# 设置图形标题\n",
    "plt.title('不同时间的交易次数分布')\n",
    "\n",
    "# 设置y轴标签\n",
    "plt.ylabel('交易次数')\n",
    "\n",
    "# 设置x轴标签\n",
    "plt.xlabel('时间')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 交易金额随时间的可视化分析\n",
    " 前面的探索分析中，我们分析了这5年跨度的总交易次数分布，绘制了交易次数随时间变化的折线分布图。可以从折线分布图中看到交易的发生时间主要集中在2016年7月至2017年12月，其余时间的交易数量极少。现在我们再从交易金额上进行观察，绘制交易金额随时间变化的折线分布图。\n",
    "\n",
    ">实训任务\n",
    ">+ 请使用`groupby()`函数统计`data`中每天的交易总金额(每笔交易金额取绝对值)，接着使用Pandas绘图机制，调用`plot()`函数绘制每天的交易总金额随时间的折线分布图。\n",
    ">  + 提示：原始`data['pay_time']`精确到秒，使用`data['pay_time'].dt.date`可以提取日期中的天。\n",
    ">+ 请设置`x`轴标签为`时间`，设置`y`轴标签为`交易金额`。\n",
    ">+ 请设置图的标题为`不同时间的交易金额分布`。\n",
    "\n",
    "和交易次数的分布类似，大部分的交易金额也主要集中在2016年7月至2017年12月，在本项目后续环节，我们选取该时间段的交易记录进行重点分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "# 绘制折线图\n",
    "abs(data['payment']).groupby(data['pay_time'].dt.date).sum().plot()\n",
    "\n",
    "# 设置图形标题\n",
    "plt.title('不同时间的交易金额分布')\n",
    "\n",
    "# 设置y轴标签\n",
    "plt.ylabel('交易金额')\n",
    "\n",
    "# 设置x轴标签\n",
    "plt.xlabel('时间')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 交易有效时段的限定\n",
    "从前面的交易数量分布折线图中，我们可以看出客户的交易主要集中在2016年7月至2017年12月，其它时间分布较少。为了使交易数据更为集中，我们选择做一个时间限定，选取从`2016-07-01`到`2017-12-31`的客户数据进行分析。\n",
    "\n",
    "Pandas中可以用`Timestamp()`函数来表示时间戳，代表某一个时间点，如`pd.Timestamp('2019-01-10')`代表`2019-01-10 00:00:00`。\n",
    "\n",
    ">实训任务\n",
    ">+ 请对`data['pay_time']`进行限定，从数据`data`中筛选出发生在2016年7月1日零点到2018年1月1日零点的交易记录，保存在DataFrame变量`data`中。\n",
    "\n",
    "对时间限定完之后，可以看到2016年7月1日到2017年12月31日的客户数据共有343万行交易流水信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 时间限定\n",
    "data = data[(data['pay_time']<=pd.Timestamp('2017-12-31')) & \n",
    "            (data['pay_time']>=pd.Timestamp('2016-07-01'))]\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 每天24小时交易次数的分布\n",
    "前面两步我们观察了总交易次数随时间的宏观变化情况，对交易的有效时间段也进行了限定。下面我们从每天的交易时段上进行分析，通过绘制柱状图的方法，统计交易数据中发生时间分布在每天24小时内的各时间段交易次数。\n",
    "\n",
    ">实训任务\n",
    ">+ 请使用`value_counts()`函数统计`data`中24小时中不同的交易次数，并使用`sort_index()`函数按照索引`0-23`进行排序。接着使用Pandas绘图机制，调用`plot()`函数画出交易次数分布的柱状图，设置颜色为`orange`，`x`轴刻度旋转`360`度。\n",
    ">  + 提示：`data['pay_time'].dt.hour`可以提取时间中的小时，再调用`value_counts()`即可统计24小时中不同的交易次数。设置颜色的参数为`color`，设置旋转刻度的参数为`rot`。\n",
    ">+ 请设置图形标题为`每天24小时的交易次数分布`。\n",
    ">+ 请设置`x`轴标签为`小时`，`y`轴标签为`交易次数`\n",
    "\n",
    "可以看到，大部分交易的时间集中在0点附近，这可能是金融机构进行清算的时间段。凌晨1点至7点交易数量较少，其余时间段交易数量分布较为均衡。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "\n",
    "# 绘制条形图\n",
    "data['pay_time'].dt.hour.value_counts().sort_index().plot(kind = 'bar', color='orange', rot=360)\n",
    "\n",
    "# 设置图形标题\n",
    "plt.title('每天24小时的交易次数分布')\n",
    "\n",
    "# 设置x轴标签\n",
    "plt.xlabel('小时')\n",
    "\n",
    "# 设置y轴标签\n",
    "plt.ylabel('交易次数')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 客户交易次数的可视化分析\n",
    "在对客户交易行为分析过程中，除了从时间维度上分析交易次数外，我们还可以从每个客户出发，可视化分析不同客户的交易分布，下面我们分析一下客户的交易次数分布。\n",
    "\n",
    "Python绘图库Seaborn中的`kdeplot()`函数可以绘制“核密度图”，用来估计数据的密度函数，从而展现数据样本本身的分布特征。\n",
    " `kdeplot()`函数的语法为：`kdeplot(data, shade=False,legend=True)`，其中：\n",
    "\n",
    "+ `data`：输入数据。\n",
    "+ `shade`：添加区域着色，默认为`False`，如果为`True`，则在KDE曲线下方的区域中着色。\n",
    "+ `legend`：添加图例，默认为`True`。\n",
    "\n",
    ">实训任务\n",
    ">+ 请使用`value_counts()`函数统计`data`每个客户的交易次数，接着调用`kdeplot()`函数画出客户交易次数的核密度图，要求在KDE曲线下方的区域中着色，且不设置图例。\n",
    ">+ 请设置`x`轴标签为`交易次数`，`y`轴标签为`频率`。\n",
    ">+ 请设置图的标题为`客户交易次数分布`。\n",
    "\n",
    "可以看到客户的交易次数从0-7000不等，分布主要集中在0-1000。在之后的指标构建中，我们要根据客户的交易记录构建指标，如果在此平台交易记录太少，我们认为此类为休眠客户。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "\n",
    "# 绘制核密度图\n",
    "sns.kdeplot(data['user_id'].value_counts(), shade=True, legend = False)\n",
    "\n",
    "# 设置x，y轴标签\n",
    "plt.xlabel('交易次数')\n",
    "plt.ylabel('频率')\n",
    "\n",
    "# 设置图的标题\n",
    "plt.title('客户交易次数分布')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 客户平均交易金额的可视化分析\n",
    " 上一步，我们通过绘制核密度图分析了不同客户的交易次数分布。接下来我们还是以核密度图的方式再统计分析一下不同客户的平均交易金额，观察整体分布。\n",
    "\n",
    ">实训任务\n",
    ">+ 请使用`groupby()`函数对`data`中的`user_id`进行分组，统计每个客户的平均交易金额，接着调用`kdeplot()`函数画出客户交易次数的核密度图，要求在KDE曲线下方的区域中着色，且不设置图例。\n",
    ">+ 请设置`x`轴标签为`平均交易金额`，`y`轴标签为`频率`。\n",
    ">+ 请设置图的标题为`客户平均交易金额的分布`。\n",
    ">  + 提示：交易金额有正有负，计算平均交易金额时，对每笔金额先取绝对值再取平均。函数的参数请参考上一节内容。\n",
    "\n",
    "可以看到客户的平均交易金额跨度较大，分布主要集中在0~10000，呈长尾分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "\n",
    "# 绘制核密度图\n",
    "sns.kdeplot(abs(data.payment).groupby(data.user_id).mean(), shade=True, legend = False)\n",
    "\n",
    "# 设置x，y轴标签\n",
    "plt.xlabel('平均交易金额')\n",
    "plt.ylabel('频率')\n",
    "\n",
    "# 设置图的标题\n",
    "plt.title('客户平均交易金额的分布')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 客户交易流入流出次数的可视化分析\n",
    "我们接着从交易次数和交易金额两个维度上对不同流向的交易做对比，以此来探索客户的行为和消费习惯。首先我们对金额流出型和金额流入型做定义，将`payment`字段取值大于零的交易记录定义为`金额流出`，取值小于零的交易记录定义为`金额流入`，对取值为0的交易记录不进行定义。接下来分别计算两种金额流动方向下的交易次数，并通过可视化的方法进行展示和分析。\n",
    "\n",
    "Python绘图库Seaborn中的`distplot()`函数融合了直方图与核密度估计功能，可以绘制“带核密度曲线的直方图”，展现数据样本的分布。\n",
    " `distplot()`函数的语法为：`distplot(data, hist=True, kde=True, ax=None)`，其中：\n",
    "\n",
    "+ `data`：输入数据。\n",
    "+ `hist`：是够绘制直方图，默认为`True`。\n",
    "+ `kde`：是否添加核密度估计曲线，默认为`True`。\n",
    "+ `ax`:  设置保存的子图。\n",
    "\n",
    ">实训任务\n",
    ">+ 请从`data`中选取出金额流入流出的交易记录，分别保存在变量`input_payment`和`output_payment`中。\n",
    ">+ 请对变量`input_payment`和`output_payment`分别进行分组，计算每个客户的流入流出次数，分别保存在变量`input_payment_count`和`output_payment_count`中。\n",
    ">+ 请使用`distplot()`函数分别绘制客户流入和流出次数的直方分布图，分别保存在子图`ax1`和`ax2`中，其余参数默认。\n",
    ">+ 请设置子图1的标题为`客户交易的流入次数分布`，子图2的标题为`客户交易的流出次数分布`。\n",
    "\n",
    "我们可以看到客户整体的流入流出次数均集中在0-1000，我们接下来再分析一下客户流入和流出金额的分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig,[ax1,ax2] = plt.subplots(1,2,figsize=(16, 5))\n",
    "\n",
    "# 定义和选取金额流入流出的交易记录\n",
    "input_payment = data[data['payment'] < 0]\n",
    "output_payment = data[data['payment'] > 0]\n",
    "\n",
    "# 计算每个客户的流入流出次数\n",
    "input_payment_count = input_payment['user_id'].value_counts()\n",
    "output_payment_count = output_payment['user_id'].value_counts()\n",
    "\n",
    "# 绘制直方图\n",
    "sns.distplot(input_payment_count, ax = ax1)\n",
    "sns.distplot(output_payment_count, ax = ax2)\n",
    "\n",
    "# 设置标题\n",
    "ax1.set_title('客户交易的流入次数分布')\n",
    "ax2.set_title('客户交易的流出次数分布')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.9 客户交易流入流出金额的可视化分析\n",
    "上一步我们对金额流出型和金额流入型的交易做定义，将`payment`字段取值大于零的交易记录定义为`金额流出`，取值小于零的交易记录定义为`金额流入`，对取值为0的交易记录不进行定义。接下来我们分别计算两种金额流动方向下的客户交易平均金额，并通过可视化的方法进行展示和分析。\n",
    "\n",
    ">实训任务\n",
    ">+ 请从`data`中选取出金额流入流出的交易记录，分别保存在变量`input_payment`和`output_payment`中。\n",
    ">+ 请对变量`input_payment`和`output_payment`分别进行分组，计算每个客户的流入流出平均金额(`交易金额不取绝对值，保留正负`)，分别保存在变量`input_payment_mount`和`output_payment_mount`中。\n",
    ">+ 请使用`distplot()`函数分别绘制客户流入和流出平均金额的直方分布图，分别保存在子图`ax1`和`ax2`中，其余参数默认。\n",
    ">+ 请设置子图1的标题为`客户交易的平均流入金额分布`，子图2的标题为`客户交易的平均流出金额分布`。\n",
    "\n",
    "我们可以看到客户整体的流入流出平均金额均集中在0-20000之间，金额流入的跨度比金额流出的跨度略大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 定义和选取金额流入流出的交易记录\n",
    "input_payment = data[data['payment'] < 0]\n",
    "output_payment = data[data['payment'] > 0]\n",
    "\n",
    "# 计算每个客户的流入流出金额\n",
    "input_payment_amount = input_payment.groupby('user_id')['payment'].mean()\n",
    "output_payment_amount = output_payment.groupby('user_id')['payment'].mean()\n",
    "\n",
    "fig,[ax1,ax2] = plt.subplots(1,2,figsize=(16, 5))\n",
    "\n",
    "# 绘制直方图\n",
    "sns.distplot(input_payment_amount, ax = ax1)\n",
    "sns.distplot(output_payment_amount, ax = ax2)\n",
    "\n",
    "# 设置标题\n",
    "ax1.set_title('客户交易的平均流入金额分布')\n",
    "ax2.set_title('客户交易的平均流出金额分布')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.10 交易附言文本预处理\n",
    "从时间和交易行为两个维度对客户进行分析之后，我们还可以挖掘交易附言文本中的信息，探索文本内容的主题，以此找寻客户标签构建的方向。在分析文本之前，我们需要先将文本提取出来并进行预处理。预处理主要有两个步骤，如图4.2所示。\n",
    "+ 中文文本分词：将中文的句子切分成有意义的词语。\n",
    "+ 去除停用词：根据事先设置好的停用词，规避掉一些特殊符号或者常用但无意义的词语。\n",
    "\n",
    "<center><img src=\".\\Pics\\Pic4_2.jpg\" width=\"600\" height=\"250\" alt=\"cursor\" align=center /></center>\n",
    "<center><font size=\"2.5px\" face=\"微软雅黑\" color=\"666666\">图4.2：分词和去除停用词</font></center>\n",
    "\n",
    "分词是自然语言处理中最基本的问题，在英文中，天然地使用空格来对句子进行分词，而中文是以字为单位，句子中所有的字连起来才能描述一个意思。只有先将文本进行分词后，才能进行词频统计、关键词提取等操作。\n",
    "\n",
    "在分词后，通常需要对词语进行去除停用词，即去除掉“以”、“让”、“用”、“不仅”、“而且”等常用但无意义的词语。但在交易数据中，文本记录已经十分简洁，很少出现传统的无意义连词，所以无须使用传统的停用词表。但是我们需要将数字、标点符号以及特殊字符(如【】、～等)输入停用词表，对分词结果进行过滤。\n",
    "\n",
    "Python中的jieba库可以进行很多中文文本的处理，包括分词、去停用词、关键词抽取和词性标注等。jieba会默认加载自带的分词词典，完成相应的分词任务。使用`jieba.cut()`函数并传入待分词的文本，使用`cut_all`参数控制选择分词模式。提供的分词模式如下：\n",
    "\n",
    "+ 精确模式：试图将句子最精确地切开，适合文本分析，默认为精确模式；`cut_all=False`。\n",
    "+ 全模式：把句子中所有可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；`cut_all=True`。\n",
    "\n",
    "例如：对“我来到清华大学”这句话进行分词，两种方式对输出结果如下：\n",
    "+ 【全模式】: 我/来到/北京/清华/清华大学/华大/大学\n",
    "+ 【精确模式】: 我/来到/北京/清华大学\n",
    "\n",
    "`jieba.cut()`函数返回的结构是一个可迭代的`generator`，可以使用`for`循环来获得分词后得到的每一个词语，也可使用`.join()`函数将序列中的元素以指定的字符进行连接。如上面的分词结果所示，`'/'.join(generator)`代表将序列中的结果以`/`进行分隔，返回结果为字符串类型。\n",
    "\n",
    "停用词表已预先读入，保存在变量`stopwords`中。由于jieba库中没有对分词结果再去除停用词的函数，我们手动定义了`del_stopwords()`函数用于匹配和删除停用词。\n",
    "\n",
    ">实训任务\n",
    ">+ 由于分词时占用计算资源较大，在题目中我们对数据进行了采样，使用`sample()`函数，设立随机种子取出20000行进行计算。（整体交易附言文本的分词已经事先完成，在之后的实训任务中可直接调用。）\n",
    ">+ 请使用`jieba.cut()`方法对每一条交易附言(`data['describe']`列)进行分词，将分词后的文本保存在`data['describe_cutted']`列中。要求分词时使用默认的精确模式，使用`join()`函数将分词后每个词之间以`空格`隔开。\n",
    ">+ 根据事先加载的停用表，对分词后的结果进行过滤。过滤停用词的函数`del_stopwords()`已默认给出，最终保存在`data['describe_cutted']`列中。\n",
    ">  + 提示：Pandas中的`apply()`函数可以对一列进行批量处理，括号中传入处理的函数(可事先定义、也可为匿名函数)。\n",
    "\n",
    "通过输出展示，我们可以看到每条交易附言已经完成了分词并且过滤掉了特殊符号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "\n",
    "# 数据采样\n",
    "data = data.sample(20000,random_state = 22)\n",
    "\n",
    "# 文本分词\n",
    "data['describe_cutted'] = data['describe'].apply(lambda x : \" \".join(jieba.cut(x)))\n",
    "\n",
    "# 过滤停用词  \n",
    "def del_stopwords(words):  \n",
    "    output = ''  \n",
    "    for word in words:  \n",
    "        if word not in stopwords:  \n",
    "            output += word\n",
    "    return output\n",
    "\n",
    "data['describe_cutted'] = data['describe_cutted'].apply(del_stopwords)\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.11 交易附言词云绘制\n",
    "对交易附言的文本进行了预处理之后，接下来我们通过绘制词云的方法对客户交易的文本数据进行可视化。\n",
    "\n",
    "词云是一种可视化描绘词语出现在文本数据中频率的方式，它主要是由随机分布在词云图的词语构成，出现频率较高的词语会以较大的形式呈现出来，而频率较低的词语则会以较小的形式呈现。对“关键词”予以视觉上的突出，形成“关键词云层”或“关键词渲染”，从而过滤掉大量的文本信息，使浏览者快速领略文本的主旨，词云示例如图4.3所示。\n",
    "\n",
    "<center><img src=\".\\Pics\\Pic4_3.png\" width=\"250\" height=\"250\" alt=\"cursor\" align=center /></center>\n",
    "<center><font size=\"2.5px\" face=\"微软雅黑\" color=\"666666\">图4.3：福尔摩斯探案集词云图</font></center>\n",
    "\n",
    "在词云图中以词频数为每个词分配权重，用字体的大小、粗细来表示权重的高低，使用不同的颜色来区分不同的词，用紧凑的布局给人以美的感受。词云绘制的整体流程如图4.4所示。\n",
    "\n",
    "<center><img src=\".\\Pics\\Pic4_4.jpg\" width=\"750\" height=\"250\" alt=\"cursor\" align=center /></center>\n",
    "<center><font size=\"2.5px\" face=\"微软雅黑\" color=\"666666\">图4.4：词云绘制流程</font></center> <br>\n",
    "\n",
    "Python词云库wordcloud可以绘制词云且功能强大，支持自定义背景颜色和图片。一个词出现的次数越多，在词云中的字体越大。创建词云对象的具体方法为：\n",
    "`wordcloud.WordCloud(parameters)`：\n",
    "+ `background_color`    ：设置背景颜色\n",
    "+ `mask`    ：设置背景图案。\n",
    "+ `font_path`    ：设置字体路径。\n",
    "+ `random_state`  ：设置随机种子。\n",
    "+ `scale`  ：按照比例进行画布放大，数值越大，产生的图片分辨率越高，字迹越清晰。\n",
    "+ `collocations` ：是否允许词语重复，默认为`True`。\n",
    "\n",
    "词云对象`wordcloud`创建好之后，使用`generate()`函数，传入绘制词云的文本数据即可绘制词云。传入的文本数据为字符串格式，我们将每条交易附言进行了拼接，保存在变量`describe_document`中，可以理解为客户交易附言组成的一个文档。\n",
    "\n",
    "## 实训任务\n",
    "+ 为了减少运算的时间，我们在绘制词云图时也进行采样，随机取出20000行进行计算。\n",
    "+ 将`data['describe_cutted']`进行拼接，保存在变量`describe_document`中，此步骤已默认给出。\n",
    "+ 请创建词云对象`wordcloud`，要求背景颜色为`white`，设置仿宋字体(相对路径为`./FangSong.ttf`)，背景图案为变量`background_Image`(已预先读入)，放大规模为`2`，不允许词语重复，随机种子设置为`30`。\n",
    "+ 请根据创建的词云对象`wordcloud`，使用`generate()`函数生成词云图。\n",
    "\n",
    "通过观察词云，可以看到文本内容主要集中在消费、转账、购物等方向。所以在之后构建客户标签体系和画像时，我们应着重从这些方面对客户进行刻画，分析其中的规律。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud         \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 数据采样\n",
    "data = data.sample(20000,random_state = 22)\n",
    "\n",
    "# 文本拼接\n",
    "describe_document = \" \".join(data['describe_cutted'])\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "# 创建词云对象\n",
    "wordcloud = WordCloud(\n",
    "    background_color = 'white',\n",
    "    mask = background_Image,\n",
    "    font_path = './FangSong.ttf',\n",
    "    random_state = 30,\n",
    "    scale = 2,\n",
    "    collocations = False,\n",
    ")\n",
    "\n",
    "# 生成词云\n",
    "wordcloud.generate(describe_document)\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.12 交易附言关键词提取\n",
    "在绘制词云时，我们把整个交易附言进行拼接，组成一个文档并保存在变量`describe_document`中。该文档中记录了所有客户的交易附言信息，接着我们根据词频的大小来规定词的权重。在自然语言处理中，不考虑句子中单词的顺序，只考虑词在这个文档中的出现次数，这叫做词袋模型。\n",
    "\n",
    "如果单纯的考虑词频会有很多缺点，有些词在此文档中出现次数较多，但在其他文档中出现得也较多，如“我们”、”你们“等指代词。这些词无法反映该文本中的主旨，所以词的权重除考虑词频之外，通常还需考虑词的重要性(tf-idf)。\n",
    "\n",
    "+ 词袋模型(bow - bag of words)：将文本表示成语料大小的高维向量，用0、1、2...等数字表示词在文本中的出现次数。\n",
    "+ 词频(tf - term frequency)：词在文本中的出现的频率，代表了词t在文档d中的重要程度。\n",
    "+ 逆文档频率(idf - inverse document frequency)：语料库中文档的总数与包含词t的文档数的比值，如果一个关键词在很少的文本中出现，通过它更容易锁定目标，那么权重应该越大。\n",
    "\n",
    "$$tf(t)= \\frac{词t在文档中出现的次数}{文档的词总数}$$\n",
    "$$idf(t)= {\\rm log}(\\frac{语料库中文档的总数}{包含词t的文档数+1})$$\n",
    "​$$tf{-}idf(d)=tf(d)·idf(d)$$\n",
    "\n",
    "\n",
    "tf-idf值越大说明这个词越重要，也可以说这个词是关键词。接下来我们尝试使用tf-idf算法提取出整个数据中交易附言部分出现的关键词。中文处理库jieba可以满足需求，关键词提取所使用逆文档频率(idf)文本语料库可以自定义设置，在这里我们使用jieba库中默认的新闻语料库。tf-idf值提取方法为：\n",
    "\n",
    "`jieba.analyse.extract_tags(sentence, topK, withWeight, allowPOS)`\n",
    "\n",
    "+ `sentence`: 待提取的文本。\n",
    "+ `topK`: 返回前多少个tf-idf权重最大的关键词，默认值为 `20`。\n",
    "+ `withWeight`: 是否一并返回关键词权重值，默认值为 `False`。\n",
    "+ `allowPOS`: 仅包括指定词性的词，默认值为空，即不筛选。\n",
    "\n",
    ">实训任务\n",
    ">+ 请使用`jieba.analyse.extract_tags()`函数提取`describe_document`中的前`50`个关键词，并返回关键词权重值，保存在变量`tags`中。\n",
    "\n",
    "可以看到，排名前50的关键词主要集中在客户的消费、转账、购物等方向，和上一步词云的结论基本相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "       \n",
    "# 提取关键词\n",
    "tags = jieba.analyse.extract_tags(\n",
    "    describe_document,\n",
    "    topK = 50,\n",
    "    withWeight = True,\n",
    ")\n",
    "\n",
    "# 输出关键词\n",
    "for tag in tags:\n",
    "    print(tag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
